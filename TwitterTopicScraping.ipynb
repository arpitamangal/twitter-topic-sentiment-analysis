{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arpitamangal/twitter-topic-sentiment-analysis/blob/main/TwitterTopicScraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZIu80-svJsu"
      },
      "outputs": [],
      "source": [
        "#!pip install -q tweepy matplotlib wordcloud\n",
        "\n",
        "import tweepy\n",
        "import time\n",
        "import requests\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "from wordcloud import STOPWORDS\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scraping"
      ],
      "metadata": {
        "id": "bR67NDE9Sbji"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-IJHkYbvKU3"
      },
      "outputs": [],
      "source": [
        "# Add Twitter API key and secret\n",
        "consumer_key = 'YOUR_CONSUMER_KEY'\n",
        "consumer_secret = 'YOUR_CONSUMER_SECRET'\n",
        "access_token = 'YOUR_ACCESS_TOKEN'\n",
        "access_token_secret = 'YOUR_ACCESS_TOKEN_SECRET'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H829b2iYvL8O"
      },
      "outputs": [],
      "source": [
        "# Handling authentication with Twitter\n",
        "\n",
        "# Authenticate with Twitter API\n",
        "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "\n",
        "# Create the API object\n",
        "# api = tweepy.API(auth)\n",
        "\n",
        "\n",
        "# Create a wrapper for the Twitter API\n",
        "api = tweepy.API(auth, wait_on_rate_limit=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KY3_MuqRyJU1"
      },
      "outputs": [],
      "source": [
        "# Helper function for handling pagination in our search and handle rate limits\n",
        "def limit_handled(cursor):\n",
        "    while True:\n",
        "        try:\n",
        "            yield cursor.next()\n",
        "        except tweepy.errors.TweepyException:\n",
        "            print (\"sleeping....\")\n",
        "            time.sleep(60 * 15)\n",
        "            continue\n",
        "        except StopIteration:\n",
        "            break\n",
        "\n",
        "# Define the term we will be using for searching tweets\n",
        "query = \"Snapdragon\"\n",
        "query = query + ' -filter:retweets'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJ8ubus6yMep"
      },
      "outputs": [],
      "source": [
        "# Define how many tweets to get from the Twitter API \n",
        "count = 18000\n",
        "\n",
        "# Search for tweets using Tweepy \n",
        "search = limit_handled(tweepy.Cursor(api.search_tweets,\n",
        "                        q=query,\n",
        "                        tweet_mode='extended',\n",
        "                        lang='en',\n",
        "                        result_type=\"recent\").items(count))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyGgtZbxyPMS",
        "outputId": "a72ce9e7-2ca9-4e45-a118-53cb3ccba330"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sleeping....\n",
            "sleeping....\n",
            "sleeping....\n",
            "sleeping....\n"
          ]
        }
      ],
      "source": [
        "# Process the results from the search using Tweepy\n",
        "tweets = []\n",
        "for result in search:\n",
        "    tweet_content = result.full_text\n",
        "    print(tweet_content)\n",
        "    # Only saving the tweet content. \n",
        "    # You could also save other attributes for each tweet like date or # of RTs.\n",
        "    tweets.append(tweet_content)\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04TGumIzySNr"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(tweets).to_csv(r\"SnapdragonRawData20220604.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analysis"
      ],
      "metadata": {
        "id": "wFGusKB5SXVs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nkJyt9biy3tO"
      },
      "outputs": [],
      "source": [
        "# Set up the API call to the Inference API to do sentiment analysis\n",
        "model = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
        "hf_token = \"YOUR_ACCESS_TOKEN\"\n",
        "API_URL = \"https://api-inference.huggingface.co/models/\" + model\n",
        "headers = {\"Authorization\": \"Bearer %s\" % (hf_token)}\n",
        "\n",
        "def analysis(data):\n",
        "    payload = dict(inputs=data, options=dict(wait_for_model=True))\n",
        "    response = requests.post(API_URL, headers=headers, json=payload)\n",
        "    return response.json()\n",
        "\n",
        "# Let's run the sentiment analysis on each tweet\n",
        "tweets_analysis = []\n",
        "for tweet in tweets:\n",
        "    try:\n",
        "        sentiment_result = analysis(tweet)[0]\n",
        "        top_sentiment = max(sentiment_result, key=lambda x: x['score']) # Get the sentiment with the higher score     \n",
        "        tweets_analysis.append({'tweet': tweet, 'sentiment': top_sentiment['label']})\n",
        "\n",
        "    except Exception as e: \n",
        "        print(e)\n",
        "        \n",
        "        \n",
        "        # Load the data in a dataframe\n",
        "pd.set_option('max_colwidth', None)\n",
        "pd.set_option('display.width', 3000) \n",
        "df = pd.DataFrame(tweets_analysis)\n",
        "\n",
        "# Show a tweet for each sentiment \n",
        "print(df[df[\"sentiment\"] == 'Positive'].head(1))\n",
        "print(df[df[\"sentiment\"] == 'Neutral'].head(1))\n",
        "print(df[df[\"sentiment\"] == 'Negative'].head(1))\n",
        "\n",
        "\n",
        "# Let's count the number of tweets by sentiments\n",
        "sentiment_counts = df.groupby(['sentiment']).size()\n",
        "print(sentiment_counts)\n",
        "\n",
        "# Let's visualize the sentiments\n",
        "fig = plt.figure(figsize=(6,6), dpi=100)\n",
        "ax = plt.subplot(111)\n",
        "sentiment_counts.plot.pie(ax=ax, autopct='%1.1f%%', startangle=270, fontsize=12, label=\"\")\n",
        "\n",
        "# Wordcloud with positive tweets\n",
        "positive_tweets = df['tweet'][df[\"sentiment\"] == 'Positive']\n",
        "stop_words = [\"https\", \"co\", \"RT\"] + list(STOPWORDS)\n",
        "positive_wordcloud = WordCloud(max_font_size=50, max_words=50, background_color=\"white\", stopwords = stop_words).generate(str(positive_tweets))\n",
        "plt.figure()\n",
        "plt.title(\"Positive Tweets - Wordcloud\")\n",
        "plt.imshow(positive_wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "# Wordcloud with negative tweets\n",
        "negative_tweets = df['tweet'][df[\"sentiment\"] == 'Negative']\n",
        "stop_words = [\"https\", \"co\", \"RT\"] + list(STOPWORDS)\n",
        "negative_wordcloud = WordCloud(max_font_size=50, max_words=50, background_color=\"white\", stopwords = stop_words).generate(str(negative_tweets))\n",
        "plt.figure()\n",
        "plt.title(\"Negative Tweets - Wordcloud\")\n",
        "plt.imshow(negative_wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "pd.DataFrame(tweets).to_csv(r\"SnapdragonRawData20220604.csv\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMUV78hmRmRfRB26sqS5b70",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}